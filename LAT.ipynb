{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim &> /dev/null\n",
        "!pip install -U pip setuptools wheel &> /dev/null\n",
        "!pip install texthero &> /dev/null\n",
        "!pip install -U spacy &> /dev/null\n",
        "!pip install natasha &> /dev/null\n",
        "!pip install stop_words &> /dev/null\n",
        "!pip install simplemma &> /dev/null"
      ],
      "metadata": {
        "id": "Pd0xfg27FyLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/stopwords-iso/stopwords-lv.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pgr88PFISjt",
        "outputId": "c0ca2a2e-1fc3-4aa8-8261-d7237c952fde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stopwords-lv'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Total 28 (delta 0), reused 0 (delta 0), pack-reused 28\u001b[K\n",
            "Unpacking objects: 100% (28/28), 5.77 KiB | 656.00 KiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZofLBZy-iWG",
        "outputId": "276eecb2-f4ec-4158-bdfd-525e824fb9f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "import io\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import texthero\n",
        "from re import sub\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "import simplemma\n",
        "#import pymorphy2\n",
        "#from pymorphy2.analyzer import MorphAnalyzer\n",
        "import gensim\n",
        "import pickle\n",
        "from gensim.models import Word2Vec\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from stop_words import get_stop_words\n",
        "import csv\n",
        "import scipy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "#morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_lv = []\n",
        "with open(r'rainis_v20180716.txt') as file:\n",
        "    for line in file:\n",
        "        corpus_lv.append(line.rstrip())"
      ],
      "metadata": {
        "id": "xMHn4pRzGNHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_lv[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7cA5hhfG-6G",
        "outputId": "9e66c3c6-c4a1-4720-a7b9-098c08780484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"<doc source='http://www.korpuss.lv/klasika/Senie/Rainis/Talnzv/tnzvsat.htm' genre='Lirika' category='Lirika+Lirika|Tālas noskaņas zilā vakarā' title='Tālas noskaņas zilā vakarā'>\",\n",
              " \"<section source='http://www.korpuss.lv/klasika/Senie/Rainis/Talnzv/tnzv1.htm' title='Ieskaņa'>\",\n",
              " 'RAINIS',\n",
              " 'TĀLAS NOSKAŅAS ZILĀ VAKARĀ',\n",
              " 'Vēlreiz garā tuvajiem mīļā dzimtenē sirsnīgus sveicienus!',\n",
              " '...Daudz simtu jūdžu tāļumā,',\n",
              " 'Aiz tīreļiem, purviem un siliem,',\n",
              " 'Guļ mana dzimtene diendusā',\n",
              " 'Tā aizsegta debešiem ziliem,',\n",
              " 'Zil-saulainiem debešu palagiem',\n",
              " 'Pret dvesmām un strāvām, un negaisiem...',\n",
              " 'Bij dziļa ziema,',\n",
              " 'Kad projām gāju',\n",
              " 'Uz tāļu zemi,',\n",
              " 'Uz svešu māju.',\n",
              " '«Nu miers virs zemes!»',\n",
              " 'Man likās skanam,',\n",
              " '«Nu labs prāts cilvēkiem!»',\n",
              " 'Pulksteņus zvanām;',\n",
              " 'Un vējus es dzirdēju']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words_dict = []\n",
        "with open(r'stopwords-lv.txt') as file:\n",
        "    for line in file:\n",
        "        stop_words_dict.append(line.rstrip())\n",
        "stop_words_dict[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ckcMFnZHFSK",
        "outputId": "389966fe-1485-494b-d40f-b00e2617cacb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aiz',\n",
              " 'ap',\n",
              " 'apakš',\n",
              " 'apakšpus',\n",
              " 'ar',\n",
              " 'arī',\n",
              " 'augšpus',\n",
              " 'bet',\n",
              " 'bez',\n",
              " 'bija',\n",
              " 'biji',\n",
              " 'biju',\n",
              " 'bijām',\n",
              " 'bijāt',\n",
              " 'būs',\n",
              " 'būsi',\n",
              " 'būsiet',\n",
              " 'būsim',\n",
              " 'būt',\n",
              " 'būšu']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text_list, stop_words):\n",
        "    # Remove elements with <> in them\n",
        "    text_list = [x for x in text_list if \"<\" not in x and \">\" not in x]\n",
        "    \n",
        "    # Make all elements lowercase\n",
        "    text_list = [x.lower() for x in text_list]\n",
        "    \n",
        "    # Get rid of punctuation\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    text_list = [x.translate(table) for x in text_list]\n",
        "    \n",
        "    # Get rid of double spaces\n",
        "    text_list = [re.sub(' +', ' ', x) for x in text_list]\n",
        "    \n",
        "    # Get rid of stopwords\n",
        "    text_list = [x for x in text_list if x not in stop_words_dict]\n",
        "\n",
        "    text_list = [re.sub(r'\\d+', '', x) for x in text_list]\n",
        "    \n",
        "    return text_list\n",
        "\n",
        "preprocessed_text = preprocess_text(corpus_lv, stop_words_dict)\n",
        "preprocessed_text[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcO64A5YKeiI",
        "outputId": "d753ed39-7e76-4651-c3d6-6f91b69e6ef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rainis',\n",
              " 'tālas noskaņas zilā vakarā',\n",
              " 'vēlreiz garā tuvajiem mīļā dzimtenē sirsnīgus sveicienus',\n",
              " 'daudz simtu jūdžu tāļumā',\n",
              " 'aiz tīreļiem purviem un siliem',\n",
              " 'guļ mana dzimtene diendusā',\n",
              " 'tā aizsegta debešiem ziliem',\n",
              " 'zilsaulainiem debešu palagiem',\n",
              " 'pret dvesmām un strāvām un negaisiem',\n",
              " 'bij dziļa ziema',\n",
              " 'kad projām gāju',\n",
              " 'uz tāļu zemi',\n",
              " 'uz svešu māju',\n",
              " '«nu miers virs zemes»',\n",
              " 'man likās skanam',\n",
              " '«nu labs prāts cilvēkiem»',\n",
              " 'pulksteņus zvanām',\n",
              " 'un vējus es dzirdēju',\n",
              " 'ņirdzīgi smejam',\n",
              " 'man paraustām svārkus']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_text = []\n",
        "romanian_numbers = [\"i\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"vii\", \"viii\", \"ix\", \"x\"]\n",
        "for line in preprocessed_text:\n",
        "    words = line.split()\n",
        "    stemmed_line = []\n",
        "    for word in words:\n",
        "        normalized_word = simplemma.lemmatize(word, lang='lv')\n",
        "        stemmed_line.append(normalized_word)\n",
        "    lemmatized_text.append(\" \".join(stemmed_line))\n",
        "    \n",
        "lemmatized_text = [s.replace(\"»\", \"\").replace(\"«\", \"\") for s in lemmatized_text if s.strip()]\n",
        "lemmatized_text = [re.sub(r'\\b\\d+\\b', '', string) for string in lemmatized_text if string not in romanian_numbers]\n",
        "\n",
        "\n",
        "# print the lemmatized text\n",
        "print(lemmatized_text[:20])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qP2U9__HQd7p",
        "outputId": "d041065e-6e15-440f-9f1c-917b0c1dc5b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['rainis', 'tālais noskaņas zils vakars', 'vēlreiz gars tuvais mīļš dzimtene sirsnīgus sveiciens', 'daudz simtu jūdze tāļumā', 'aiz tīrelis purvs un sils', 'gulēt mans dzimtene diendusā', 'tā aizsegta debešiem zilais', 'zilsaulainde debešu palags', 'pret dvesmām un strāvām un negaiss', 'bij dziļš ziema', 'kad projām iet', 'uz tāļu zeme', 'uz svešs māja', 'nu miers virs zemes', 'es likties skanēt', 'nu labs prāts cilvēks', 'pulkstenis zvanām', 'un vējš es dzirdēt', 'ņirdzīgi smejam', 'es paraustām svārki']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_list_to_csv(list_, file_name):\n",
        "    with open(file_name, 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        for row in list_:\n",
        "            writer.writerow([row])\n",
        "save_list_to_csv(lemmatized_text, 'stemmed_text_lv_new.csv')"
      ],
      "metadata": {
        "id": "Mpea4P0sLL-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_text[:30]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GcmKD2lQc16",
        "outputId": "8bff7bca-d311-4fda-9d91-d1dedfd7c54e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rainis',\n",
              " 'tālais noskaņas zils vakars',\n",
              " 'vēlreiz gars tuvais mīļš dzimtene sirsnīgus sveiciens',\n",
              " 'daudz simtu jūdze tāļumā',\n",
              " 'aiz tīrelis purvs un sils',\n",
              " 'gulēt mans dzimtene diendusā',\n",
              " 'tā aizsegta debešiem zilais',\n",
              " 'zilsaulainde debešu palags',\n",
              " 'pret dvesmām un strāvām un negaiss',\n",
              " 'bij dziļš ziema',\n",
              " 'kad projām iet',\n",
              " 'uz tāļu zeme',\n",
              " 'uz svešs māja',\n",
              " 'nu miers virs zemes',\n",
              " 'es likties skanēt',\n",
              " 'nu labs prāts cilvēks',\n",
              " 'pulkstenis zvanām',\n",
              " 'un vējš es dzirdēt',\n",
              " 'ņirdzīgi smejam',\n",
              " 'es paraustām svārki',\n",
              " 'un garš skrejam',\n",
              " 'sniegt putens griezās',\n",
              " 'es acs sist',\n",
              " 'lēns uz Ziemsvētki',\n",
              " 'eglītēm krist',\n",
              " 'ak miers virs zeme',\n",
              " 'un gars dusa',\n",
              " 'un ziema ziema',\n",
              " 'sirds sāpēt kust',\n",
              " 'es sirds ziedons']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stem_corp= pd.read_csv('stemmed_text_lv_new.csv')"
      ],
      "metadata": {
        "id": "ZDJFaq0-wQ24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(min_df=5)\n",
        "\n",
        "data_vectorized = vectorizer.fit_transform(stem_corp['rainis'][::3])"
      ],
      "metadata": {
        "id": "w7lcnx3puu_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = data_vectorized.toarray()\n",
        "A = A.astype(float)"
      ],
      "metadata": {
        "id": "8q4i3-erv-Qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = sorted(vectorizer.vocabulary_.keys())"
      ],
      "metadata": {
        "id": "cPFQMcdCwM-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(feature_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2ltRZudw2oe",
        "outputId": "12353b96-68dc-40c5-a5e5-cf0fc5395aed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7948"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tn2dAdbUy839",
        "outputId": "4ff069d8-a0fb-43eb-dd37-bafc7ca131a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(51055, 7948)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "K = 10\n",
        "u, s, vt = scipy.sparse.linalg.svds(A.T, k=K)"
      ],
      "metadata": {
        "id": "ZQUfwgW_y9h9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A.T.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhnC2SNpzVYc",
        "outputId": "cd7415f0-a605-44f6-c147-dbc88beb122d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7948, 51055)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "u.shape, s.shape, vt.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxjGZlOhzX60",
        "outputId": "d3c89c30-d0a1-48eb-dcdb-76166c05d7d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((7948, 10), (10,), (10, 51055))"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('lv_vt_' + str(K) +  '.pickle', 'wb') as f:\n",
        "  pickle.dump(vt, f)\n",
        "with open('lv_u_' + str(K) + '.pickle', 'wb') as f:\n",
        "  pickle.dump(u, f)\n",
        "with open('lv_s_.' + str(K) + 'pickle', 'wb') as f:\n",
        "  pickle.dump(s, f)"
      ],
      "metadata": {
        "id": "bXCfUPYpzcDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(feature_names[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXUZdIk6zt7R",
        "outputId": "30e97eef-7a3c-4d97-afe7-fb47d59220f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['aba', 'abi', 'abonents', 'abonēt', 'absolūti', 'aco', 'acs', 'actiņa', 'actiņas', 'actiņām', 'acumirklis', 'adata', 'adrese', 'advokatūrā', 'advokāts', 'adīt', 'afelds', 'aforisms', 'agrs', 'agrākais', 'agrāks', 'aicināt', 'aicināts', 'aidā', 'aila', 'ailā', 'aina', 'ainas', 'ainava', 'ainu', 'ainām', 'ainās', 'aita', 'aiva', 'aiz', 'aizbrauc', 'aizbrauca', 'aizbraucis', 'aizbraukt', 'aizbraukšu', 'aizbēg', 'aizdevums', 'aizdomas', 'aizdot', 'aizdzen', 'aizejošs', 'aizgrieža', 'aizgājis', 'aiziedams', 'aiziet']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):  \n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stop words\n",
        "    stop_words = stop_words_dict\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    # Perform morphological analysis and lemmatization\n",
        "    lemmatized_tokens = []\n",
        "    for token in tokens:\n",
        "        lemma = simplemma.lemmatize(token, lang='lv')\n",
        "        lemmatized_tokens.append(lemma)\n",
        "    # Join the preprocessed tokens back into a single string\n",
        "    preprocessed_text = ' '.join(lemmatized_tokens)\n",
        "    return preprocessed_text\n",
        "\n",
        "def convertTextToEmbeddingSeries(text, dict_words, emb):\n",
        "    wtext = preprocess_text(text)\n",
        "    wdata = wtext.split(' ')\n",
        "    word2ind = dict(zip(dict_words, np.arange(len(dict_words))))\n",
        "    embText = []\n",
        "    for word in wdata:\n",
        "        ind = word2ind.get(word)\n",
        "        if ind: embText.append(emb[ind])\n",
        "    return np.array(embText)"
      ],
      "metadata": {
        "id": "iY_icS3Qz785"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_text = preprocess_text('Atnācu no ielas, atradu kaķi un sāku glāstīt viņa kažoku!')\n",
        "preprocessed_text"
      ],
      "metadata": {
        "id": "4ffzrLcwz_Zq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "6f9e9ee8-1305-472b-d938-03fde7cbe68f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'atnācu iela atrast kaķe sākt glāstīt viņa kažoks'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textSeries = convertTextToEmbeddingSeries(preprocessed_text,\n",
        "                                          feature_names,\n",
        "                                          u)"
      ],
      "metadata": {
        "id": "8OUegFKs0Bma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "textSeries.shape"
      ],
      "metadata": {
        "id": "GLrWIxXj0CQ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a12614ca-aaca-4b10-b2fc-0db7d9cd3096"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textSeries"
      ],
      "metadata": {
        "id": "3iHxw9Tu0FkV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4007160d-bba7-4bc3-d35a-7eeeecdb1c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-7.44353949e-04,  5.46467283e-04, -9.69686382e-04,\n",
              "         1.04959102e-03,  2.61089572e-04, -2.35160872e-04,\n",
              "        -1.96949946e-05,  4.82218788e-04,  2.13832037e-04,\n",
              "         7.74820069e-05],\n",
              "       [-5.67993671e-04,  1.55899286e-03,  3.69818498e-04,\n",
              "         2.85639079e-03, -9.51586479e-05, -4.91955126e-04,\n",
              "         2.75490797e-04, -1.48344829e-04,  1.92822885e-03,\n",
              "         9.08187656e-04],\n",
              "       [-1.38086777e-05,  2.33192415e-03,  1.34745172e-03,\n",
              "         1.30362757e-03,  3.17836809e-04, -4.88344403e-05,\n",
              "         9.53449398e-05,  3.59607650e-03,  1.73060886e-03,\n",
              "         1.83727405e-03],\n",
              "       [-1.39013400e-03,  8.15709148e-05, -5.51433746e-04,\n",
              "        -8.70441082e-05, -1.32905799e-03, -1.94448511e-04,\n",
              "         1.64459824e-04,  2.70990863e-04,  3.21271392e-04,\n",
              "         2.53756673e-04],\n",
              "       [-1.92160472e-02, -5.81001224e-03, -8.32449303e-03,\n",
              "         1.52680014e-02,  9.66549489e-03,  9.33756998e-03,\n",
              "         5.24048927e-03,  1.47768684e-02,  9.10273327e-03,\n",
              "         9.70380022e-03],\n",
              "       [-8.17716033e-02,  1.36321479e-01,  6.76311609e-02,\n",
              "         5.32770023e-02,  5.66045427e-03,  6.18842833e-02,\n",
              "         1.13112284e-02,  1.01280216e-02,  9.82879936e-02,\n",
              "         3.46239911e-02],\n",
              "       [-2.28196933e-03,  1.39063437e-03, -3.34855519e-03,\n",
              "         1.50657993e-03, -6.58792214e-04, -7.32523851e-04,\n",
              "        -4.66659815e-04,  6.70282911e-04,  9.45416091e-04,\n",
              "         2.86077981e-04]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}